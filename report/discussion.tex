\documentclass[Interploate_hadwritten_Digits.tex]{subfiles}

\begin{document}
	\subsection{Interpretation Resultate}
	Aus den Resultaten lassen sich verschiedene Schlüsse ziehen. Zum Ersten zeigen die Resultate im Kapitel \ref{sec:results_classification}, dass das gewählte Modell für das Neuronale Netzwerk einen Trainingsprozess hat, der nicht immer zu einem verwertbaren Resultat führt. Eine mögliche Erklärung  dafür könnte der Verlust der Nachbarschaftsbeziehung von Pixeln durch die Umformung des Bildes in einen Vektor sein. Das Neuronale Netz muss diese Beziehung zuerst erlernen.
	
	Weiter haben die Resultate im Kapitel \ref{sec:results_compression} gezeigt, dass die Komprimierung von Neuronalen Netzen in eine Matrix schwer zu realisieren ist. Der Grund dafür ist wahrscheinlich die nichtlineare Aktivierungsfunktion, welche ein wichtiger Bestandteil der Funktion des Neurons ist. Diese Nicht-Linearität verhindert, dass die einzelnen Schichten in Netz zu einer Matrix zusammengefasst werden können\footnote{\cite{standford-cs231n-convolutional_neuronal_networks_for_visual_recognition}}. Das Verwenden einer linearen Funktion als Aktivierung hätte einen negativen Einfluss auf die Leistungsfähigkeit des Neurons.
	
	Eine weitere Schwierigkeit zeigen die Resultate im Kapitel \ref{sec:results_error_inverse} auf. Die verwendete Aktivierungsfunktion hat die Eigenschaft, alle Zahlen $ \in \mathbb{R} $ auf den Bereich $ [0, 1] $ einzuschränken. Dadurch amplifiziert das Inverse der Funktion aber auch kleine Änderungen, welche durch Rundungsfehler entstehen können, und lässt diese signifikant werden. Bei einer gewissen Anzahl Gleitkommazahloperationen sind solche Rundungsfehler unvermeidbar, und durch das Verwenden von Matrixmultiplikationen werden solche Fehler durch die ganze Matrix propagiert. Dies verunmöglicht das einfache zurückrechnen durch das Neuronalen Netzwerk.
	
	Um trotz der Rundungsfehler der Gleitkommazahlen Ziffern im Zielraum zu interpolieren, wurde der Ansatz der Approximation des Inputbildes gewählt. Im Kapitel \ref{sec:results_appriximation} ist zu sehen, dass die Fehler der Kreuzentropie bei der Approximation von idealen Ziffern mit bis zu 1.5 relativ vielversprechend ist. Jedoch lassen sich in den generierten Bildern keine Ziffern erkennen. Bei den interpolierten Ziffern ist der Fehler zu hoch, um gute Ergebnisse zu erwarten. 
	
	\subsection{Weiterführende Arbeiten}
	Viele Fehler sind vermutlich auf den Verlust der Nachbarschaftsbeziehung der einzelnen Pixel zurückzuführen. Diese Problematik könnte sich mit der Verwendung eines Convolutional Neural Networks entschärfen lassen. Solange nur Faltungen verwendet würden, könnten diese durch eine Fourier Transformation invertiert werden\footnote{\cite{weisstein-convolution_theorem}}. Sollte am Schluss noch ein Fully Connected Layer verwendet werden, müsste sichergestellt werden, dass dieser klein genug ist um die Wahrscheinlichkeit für einen signifikanten Gleitkommafehler gering zu halten.
	Ein anderer Ansatz wäre die Pixel komplett loszuwerden und die Ziffer als Linie mathematisch zu beschreiben. Eine Möglichkeit dafür wären die ``Snakes: Active contour models''\footnote{\cite{kass-snake}}. So wäre das Resultat bei einer Invertierung auch garantiert eine geometrische Form.
	
	\subsection{Fazit}
	Die Frage, ob Vorgänge innerhalb eines Neuronalen Netzes mithilfe von Rückrechnung durch ein Neuronale Netz visualisieren lassen, kann diese Arbeit nicht vollständig klären. Gezeigt hat sich, dass eine Invertierung eines Feed Forward Neuronalen Netzes sich nicht für eine Visualisierung eignet, wenn das Netzwerk etwas grösser wird. Die Resultate der Approximation legen jedoch nahe, dass ein Feed Forward Neuronales Netzwerk kein gutes Verständnis von Ziffern aus reinen Pixeln erlernt hat. Dies wird weiter durch den letzten Versuch dieser Arbeit, für den Start der Approximation Bilder aus dem Trainingsset zu verwenden, unterstützt. Bei diesem Versuch konnte festgestellt werden, dass keine Veränderung am Inputbild vorgenommen wird, auch wenn eine andere Ziffer als Zielvektor angegeben wurde. So scheint es, als fände das trainierte Netz ein lokales Maximum, welches gut genug ist, um die meisten Ziffern zu erkennen. Von einem Verständnis für Ziffern kann daher nicht die Rede sein.
	
\end{document}